"""
AIæ•°æ®æå–å™¨ - é›†æˆå¤šç§LLMæä¾›å•†è¿›è¡Œæ™ºèƒ½æ•°æ®æå–
"""

import json
import yaml
import logging
import asyncio
import aiohttp
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
import os

class AIExtractor:
    """AIæ•°æ®æå–å™¨ç±»ï¼Œæ”¯æŒå¤šç§LLMæä¾›å•†"""
    
    def __init__(self, config_path: str = None):
        """
        åˆå§‹åŒ–AIæå–å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æŸ¥æ‰¾
        """
        # å¦‚æœæ²¡æœ‰æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„ï¼Œåˆ™è‡ªåŠ¨æŸ¥æ‰¾
        if config_path is None:
            # è·å–å½“å‰æ–‡ä»¶çš„ç›®å½•
            current_dir = Path(__file__).parent
            # æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„é…ç½®æ–‡ä»¶
            project_root = current_dir.parent
            config_path = project_root / "config" / "ai_config.yaml"
        
        self.config_path = str(config_path)
        self.logger = self._setup_logging()  # å…ˆè®¾ç½®æ—¥å¿—
        self.config = self._load_config()    # å†åŠ è½½é…ç½®
        
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            config_path = Path(self.config_path)
            if not config_path.exists():
                self.logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
                self.logger.info("ğŸ”§ ä½¿ç”¨é»˜è®¤é…ç½®")
                return self._get_default_config()
            
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                self.logger.info(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
                return config
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            self.logger.info("ğŸ”§ ä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
    
    def _get_default_config(self) -> dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "llm": {
                "default_provider": "openai",
                "openai": {
                    "api_key": "",
                    "model": "gpt-3.5-turbo",
                    "base_url": "https://api.openai.com/v1",
                    "temperature": 0.1,
                    "max_tokens": 4000
                },
                "azure_openai": {
                    "api_key": "",
                    "endpoint": "",
                    "api_version": "2024-02-15-preview",
                    "deployment_name": "",
                    "temperature": 0.1,
                    "max_tokens": 4000
                },
                "local_llm": {
                    "base_url": "http://localhost:11434",
                    "model": "llama2",
                    "temperature": 0.1,
                    "max_tokens": 4000
                },
                "qwen": {
                    "api_key": "",
                    "model": "qwen-turbo",
                    "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
                    "temperature": 0.1,
                    "max_tokens": 4000
                }
            },
            "extraction": {
                "default_mode": "structured_data",
                "prompts": {
                    "structured_data": "è¯·ä»ä»¥ä¸‹å†…å®¹ä¸­æå–ç»“æ„åŒ–æ•°æ®ï¼Œä»¥JSONæ ¼å¼è¿”å›ï¼š\n\n{content}",
                    "summary": "è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹çš„ä¸»è¦è§‚ç‚¹ï¼š\n\n{content}",
                    "keywords": "è¯·ä»ä»¥ä¸‹å†…å®¹ä¸­æå–å…³é”®è¯ï¼š\n\n{content}",
                    "entities": "è¯·ä»ä»¥ä¸‹å†…å®¹ä¸­æå–å®ä½“ï¼ˆäººåã€åœ°åã€ç»„ç»‡ç­‰ï¼‰ï¼š\n\n{content}",
                    "sentiment": "è¯·åˆ†æä»¥ä¸‹å†…å®¹çš„æƒ…æ„Ÿå€¾å‘ï¼š\n\n{content}"
                },
                "max_content_length": 8000
            },
            "output": {
                "save_results": True,
                "output_format": "json",
                "filename_pattern": "ai_analysis_{timestamp}_{mode}.json"
            },
            "logging": {
                "level": "INFO",
                "file": "logs/ai_extraction.log"
            }
        }
    
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        logger = logging.getLogger("AIExtractor")
        logger.setLevel(logging.INFO)
        
        # å¦‚æœå·²ç»æœ‰å¤„ç†å™¨ï¼Œåˆ™ä¸é‡å¤æ·»åŠ 
        if logger.handlers:
            return logger
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        log_file = log_dir / "ai_extraction.log"
        handler = logging.FileHandler(log_file, encoding='utf-8')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        
        logger.addHandler(handler)
        logger.addHandler(console_handler)
        
        return logger
    
    def _build_prompt(self, content: str, mode: str) -> str:
        """
        æ„å»ºæç¤ºè¯
        
        Args:
            content: è¦å¤„ç†çš„å†…å®¹
            mode: æå–æ¨¡å¼
            
        Returns:
            æ„å»ºå¥½çš„æç¤ºè¯
        """
        # è·å–æç¤ºè¯æ¨¡æ¿
        prompts = self.config.get("extraction", {}).get("prompts", {})
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©æç¤ºè¯æ¨¡æ¿
        if mode == "structured_data":
            template = prompts.get("structured_data", 
                "è¯·ä»ä»¥ä¸‹å†…å®¹ä¸­æå–ç»“æ„åŒ–æ•°æ®ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€å…³é”®ä¿¡æ¯ã€æ•°æ®ç­‰ï¼š\n\n{content}")
        elif mode == "content_summary":
            template = prompts.get("content_summary", 
                "è¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œæ€»ç»“ï¼Œæå–ä¸»è¦è§‚ç‚¹å’Œæ ¸å¿ƒä¿¡æ¯ï¼š\n\n{content}")
        elif mode == "key_points":
            template = prompts.get("key_points", 
                "è¯·ä»ä»¥ä¸‹å†…å®¹ä¸­æå–å…³é”®ç‚¹ï¼Œä»¥åˆ—è¡¨å½¢å¼å‘ˆç°ï¼š\n\n{content}")
        elif mode == "entities":
            template = prompts.get("entities", 
                "è¯·ä»ä»¥ä¸‹å†…å®¹ä¸­è¯†åˆ«å’Œæå–å®ä½“ä¿¡æ¯ï¼ˆäººåã€åœ°åã€æœºæ„åç­‰ï¼‰ï¼š\n\n{content}")
        elif mode == "sentiment":
            template = prompts.get("sentiment", 
                "è¯·åˆ†æä»¥ä¸‹å†…å®¹çš„æƒ…æ„Ÿå€¾å‘å’Œæ€åº¦ï¼š\n\n{content}")
        else:
            template = "è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š\n\n{content}"
        
        # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…è¶…å‡ºAPIé™åˆ¶
        max_content_length = 4000
        if len(content) > max_content_length:
            content = content[:max_content_length] + "..."
            self.logger.info(f"å†…å®¹è¿‡é•¿ï¼Œå·²æˆªå–å‰{max_content_length}ä¸ªå­—ç¬¦")
        
        return template.format(content=content)

    async def extract_with_openai(self, content: str, mode: str = "structured_data") -> Dict[str, Any]:
        """
        ä½¿ç”¨OpenAIè¿›è¡Œæ•°æ®æå–
        
        Args:
            content: è¦æå–çš„å†…å®¹
            mode: æå–æ¨¡å¼
            
        Returns:
            æå–ç»“æœ
        """
        try:
            openai_config = self.config.get("openai", {})
            api_key = openai_config.get("api_key")
            base_url = openai_config.get("base_url", "https://api.openai.com/v1")
            model = openai_config.get("model", "gpt-3.5-turbo")
            
            if not api_key or api_key == "your-openai-api-key-here":
                self.logger.warning("OpenAI APIå¯†é’¥æœªé…ç½®ï¼Œè·³è¿‡AIæå–")
                return {"error": "APIå¯†é’¥æœªé…ç½®"}
            
            # è·å–æç¤ºè¯æ¨¡æ¿
            prompt_template = self.config.get("extraction", {}).get("prompts", {}).get(mode, "")
            if not prompt_template:
                prompt_template = "è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š\n{content}"
            
            prompt = prompt_template.format(content=content[:4000])  # é™åˆ¶å†…å®¹é•¿åº¦
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": model,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": openai_config.get("max_tokens", 2000),
                "temperature": openai_config.get("temperature", 0.1)
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{base_url}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        ai_response = result["choices"][0]["message"]["content"]
                        
                        self.logger.info(f"OpenAIæå–æˆåŠŸï¼Œæ¨¡å¼: {mode}")
                        return {
                            "success": True,
                            "mode": mode,
                            "provider": "openai",
                            "result": ai_response,
                            "timestamp": datetime.now().isoformat()
                        }
                    else:
                        error_text = await response.text()
                        self.logger.error(f"OpenAI APIè¯·æ±‚å¤±è´¥: {response.status} - {error_text}")
                        return {"error": f"APIè¯·æ±‚å¤±è´¥: {response.status}"}
                        
        except Exception as e:
            self.logger.error(f"OpenAIæå–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {"error": str(e)}
    
    async def extract_with_local_llm(self, content: str, mode: str = "structured_data") -> Dict[str, Any]:
        """
        ä½¿ç”¨æœ¬åœ°LLMè¿›è¡Œæ•°æ®æå–ï¼ˆå¦‚Ollamaï¼‰
        
        Args:
            content: è¦æå–çš„å†…å®¹
            mode: æå–æ¨¡å¼
            
        Returns:
            æå–ç»“æœ
        """
        try:
            local_config = self.config.get("local_llm", {})
            base_url = local_config.get("base_url", "http://localhost:11434")
            model = local_config.get("model", "llama2")
            
            # è·å–æç¤ºè¯æ¨¡æ¿
            prompt_template = self.config.get("extraction", {}).get("prompts", {}).get(mode, "")
            if not prompt_template:
                prompt_template = "è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š\n{content}"
            
            prompt = prompt_template.format(content=content[:4000])  # é™åˆ¶å†…å®¹é•¿åº¦
            
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": local_config.get("temperature", 0.1),
                    "num_predict": local_config.get("max_tokens", 2000)
                }
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{base_url}/api/generate",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=120)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        ai_response = result.get("response", "")
                        
                        self.logger.info(f"æœ¬åœ°LLMæå–æˆåŠŸï¼Œæ¨¡å¼: {mode}")
                        return {
                            "success": True,
                            "mode": mode,
                            "provider": "local_llm",
                            "result": ai_response,
                            "timestamp": datetime.now().isoformat()
                        }
                    else:
                        error_text = await response.text()
                        self.logger.error(f"æœ¬åœ°LLMè¯·æ±‚å¤±è´¥: {response.status} - {error_text}")
                        return {"error": f"æœ¬åœ°LLMè¯·æ±‚å¤±è´¥: {response.status}"}
                        
        except Exception as e:
            self.logger.error(f"æœ¬åœ°LLMæå–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {"error": str(e)}
    
    async def extract_with_qwen(self, content: str, mode: str = "structured_data") -> Dict[str, Any]:
        """
        ä½¿ç”¨é€šä¹‰åƒé—®è¿›è¡Œæ•°æ®æå–
        
        Args:
            content: è¦æå–çš„å†…å®¹
            mode: æå–æ¨¡å¼
            
        Returns:
            æå–ç»“æœ
        """
        try:
            qwen_config = self.config.get("qwen", {})
            api_key = qwen_config.get("api_key")
            base_url = qwen_config.get("base_url", "https://dashscope.aliyuncs.com/compatible-mode/v1")
            model = qwen_config.get("model", "qwen-plus")
            
            if not api_key or api_key == "your-qwen-api-key-here":
                self.logger.warning("QWEN APIå¯†é’¥æœªé…ç½®ï¼Œè·³è¿‡AIæå–")
                return {"error": "APIå¯†é’¥æœªé…ç½®"}
            
            # è·å–æç¤ºè¯æ¨¡æ¿
            prompt_template = self.config.get("extraction", {}).get("prompts", {}).get(mode, "")
            if not prompt_template:
                prompt_template = "è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š\n{content}"
            
            prompt = prompt_template.format(content=content[:4000])  # é™åˆ¶å†…å®¹é•¿åº¦
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": model,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": qwen_config.get("max_tokens", 2000),
                "temperature": qwen_config.get("temperature", 0.1)
            }
            
            # ç¡®ä¿URLæ ¼å¼æ­£ç¡®
            if not base_url.endswith("/chat/completions"):
                if base_url.endswith("/"):
                    base_url = base_url + "chat/completions"
                else:
                    base_url = base_url + "/chat/completions"
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    base_url,
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        ai_response = result["choices"][0]["message"]["content"]
                        
                        self.logger.info(f"QWENæå–æˆåŠŸï¼Œæ¨¡å¼: {mode}")
                        return {
                            "success": True,
                            "mode": mode,
                            "provider": "qwen",
                            "result": ai_response,
                            "timestamp": datetime.now().isoformat()
                        }
                    else:
                        error_text = await response.text()
                        self.logger.error(f"QWEN APIè¯·æ±‚å¤±è´¥: {response.status} - {error_text}")
                        return {"error": f"APIè¯·æ±‚å¤±è´¥: {response.status}"}
                        
        except Exception as e:
            self.logger.error(f"QWENæå–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {"error": str(e)}
    
    async def extract(self, content: str, mode: str = None, provider: str = None) -> Dict[str, Any]:
        """
        æ‰§è¡ŒAIæ•°æ®æå–
        
        Args:
            content: è¦æå–çš„å†…å®¹
            mode: æå–æ¨¡å¼ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å¼
            provider: LLMæä¾›å•†ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æä¾›å•†
            
        Returns:
            æå–ç»“æœ
        """
        if not mode:
            mode = self.config.get("extraction", {}).get("default_mode", "structured_data")
        
        if not provider:
            provider = self.config.get("default_provider", "openai")
        
        self.logger.info(f"ğŸš€ å¼€å§‹AIæå– - æä¾›å•†: {provider}, æ¨¡å¼: {mode}, å†…å®¹é•¿åº¦: {len(content)}")
        
        # æ ¹æ®æä¾›å•†é€‰æ‹©ç›¸åº”çš„æå–æ–¹æ³•
        if provider == "openai":
            result = await self.extract_with_openai(content, mode)
        elif provider == "local_llm":
            result = await self.extract_with_local_llm(content, mode)
        elif provider == "qwen":
            result = await self.extract_with_qwen(content, mode)
        else:
            error_msg = f"âŒ ä¸æ”¯æŒçš„æä¾›å•†: {provider}"
            self.logger.error(error_msg)
            return {"success": False, "error": error_msg}
        
        # è®°å½•æå–ç»“æœ
        if result.get("success"):
            self.logger.info(f"âœ… AIæå–æˆåŠŸ - æä¾›å•†: {provider}, æ¨¡å¼: {mode}")
        else:
            self.logger.error(f"âŒ AIæå–å¤±è´¥ - æä¾›å•†: {provider}, é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        return result
    
    def save_result(self, result: Dict[str, Any], filename: str = None) -> str:
        """
        ä¿å­˜æå–ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            result: æå–ç»“æœ
            filename: æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            if not filename:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                mode = result.get("mode", "unknown")
                filename = f"ai_extract_{mode}_{timestamp}.json"
            
            # ç¡®ä¿æ–‡ä»¶åä»¥.jsonç»“å°¾
            if not filename.endswith('.json'):
                filename += '.json'
            
            filepath = Path(filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"æå–ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
            return str(filepath)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return ""
    
    def get_available_modes(self) -> List[str]:
        """è·å–å¯ç”¨çš„æå–æ¨¡å¼åˆ—è¡¨"""
        return list(self.config.get("extraction", {}).get("prompts", {}).keys())
    
    def get_available_providers(self) -> List[str]:
        """è·å–å¯ç”¨çš„LLMæä¾›å•†åˆ—è¡¨"""
        providers = []
        
        # æ£€æŸ¥å„ä¸ªæä¾›å•†çš„é…ç½®
        if "openai" in self.config and self.config["openai"].get("api_key"):
            providers.append("openai")
        
        if "azure_openai" in self.config and self.config["azure_openai"].get("api_key"):
            providers.append("azure_openai")
            
        if "local_llm" in self.config:
            providers.append("local_llm")
            
        if "qwen" in self.config and self.config["qwen"].get("api_key"):
            providers.append("qwen")
        
        self.logger.info(f"ğŸ“‹ å¯ç”¨çš„LLMæä¾›å•†: {providers}")
        return providers