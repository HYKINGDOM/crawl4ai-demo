"""
HTTPæœåŠ¡å™¨ - æä¾›ç½‘é¡µçˆ¬å–å’ŒAIåˆ†æçš„APIæ¥å£
æ”¯æŒé€šè¿‡HTTPæ¥å£è°ƒç”¨ç½‘é¡µçˆ¬å–å’ŒAIæå–åŠŸèƒ½
é›†æˆMinIOå­˜å‚¨å’ŒPostgreSQLæ•°æ®åº“
"""
import asyncio
import json
import logging
import sys
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional
from urllib.parse import urlparse

from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi import Request
from pydantic import BaseModel, HttpUrl
import uvicorn

# æ·»åŠ utilsç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from utils.ai_extractor import AIExtractor
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator

# å¯¼å…¥æ•°æ®åº“å’Œå­˜å‚¨é…ç½®
from config.database_config import db_manager, get_db, init_database
from config.minio_config import minio_manager
from models.database_models import CrawlTask, CrawlFile


class CrawlRequest(BaseModel):
    """çˆ¬å–è¯·æ±‚æ¨¡å‹"""
    url: HttpUrl
    content_source: str = "cleaned_html"  # cleaned_html, raw_html, fit_html
    ai_modes: list[str] = ["structured_data", "content_summary", "key_points"]
    save_files: bool = True


class CrawlResponse(BaseModel):
    """çˆ¬å–å“åº”æ¨¡å‹"""
    success: bool
    url: str
    timestamp: str
    markdown_content: Optional[str] = None
    ai_results: Optional[Dict[str, Any]] = None
    storage_info: Optional[Dict[str, Any]] = None  # æ›¿æ¢saved_files
    error: Optional[str] = None


class ContentSourceServer:
    """ç½‘é¡µå†…å®¹çˆ¬å–å’ŒAIåˆ†æHTTPæœåŠ¡å™¨"""
    
    def __init__(self, port: int = 8000, host: str = "0.0.0.0"):
        """
        åˆå§‹åŒ–æœåŠ¡å™¨
        
        Args:
            port: æœåŠ¡å™¨ç«¯å£
            host: æœåŠ¡å™¨ä¸»æœºåœ°å€
        """
        self.port = port
        self.host = host
        
        # è®¾ç½®æ—¥å¿—
        self.logger = self._setup_logging()
        self.logger.info(f"ğŸ”§ åˆå§‹åŒ–ContentSourceServerï¼Œç«¯å£: {port}, ä¸»æœº: {host}")
        
        # åˆ›å»ºFastAPIåº”ç”¨
        self.app = FastAPI(
            title="Crawl4AI HTTPæœåŠ¡å™¨",
            description="æä¾›ç½‘é¡µçˆ¬å–å’ŒAIåˆ†æçš„APIæ¥å£ï¼Œé›†æˆMinIOå­˜å‚¨å’ŒPostgreSQLæ•°æ®åº“",
            version="2.0.0"
        )
        
        # åˆå§‹åŒ–AIæå–å™¨
        self.logger.info("ğŸ¤– åˆå§‹åŒ–AIæå–å™¨...")
        self.ai_extractor = AIExtractor()
        
        # è®¾ç½®æ¨¡æ¿å’Œé™æ€æ–‡ä»¶
        self.logger.info("ğŸ“ è®¾ç½®æ¨¡æ¿å’Œé™æ€æ–‡ä»¶...")
        self.templates = Jinja2Templates(directory="templates")
        
        # åˆå§‹åŒ–æ•°æ®åº“å’Œå­˜å‚¨
        self.logger.info("ğŸ’¾ åˆå§‹åŒ–æ•°æ®åº“å’Œå­˜å‚¨...")
        self._init_database_and_storage()
        
        # è®¾ç½®è·¯ç”±
        self.logger.info("ğŸ›£ï¸ è®¾ç½®APIè·¯ç”±...")
        self._setup_routes()
        
        # æŒ‚è½½é™æ€æ–‡ä»¶
        self.logger.info("ğŸ“‚ æŒ‚è½½é™æ€æ–‡ä»¶...")
        self.app.mount("/static", StaticFiles(directory="static"), name="static")
        
        self.logger.info("âœ… ContentSourceServeråˆå§‹åŒ–å®Œæˆ")
        
    def _init_database_and_storage(self):
        """åˆå§‹åŒ–æ•°æ®åº“å’ŒMinIOå­˜å‚¨"""
        try:
            # åˆå§‹åŒ–æ•°æ®åº“
            init_database()
            self.logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
            
            # MinIOå­˜å‚¨æ¡¶åœ¨åˆå§‹åŒ–æ—¶å·²ç»åˆ›å»ºï¼Œè¿™é‡Œåªéœ€è¦æµ‹è¯•è¿æ¥
            if minio_manager.test_connection():
                self.logger.info("âœ… MinIOå­˜å‚¨è¿æ¥æˆåŠŸ")
            else:
                raise Exception("MinIOè¿æ¥æµ‹è¯•å¤±è´¥")
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åº“æˆ–å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        logger = logging.getLogger("ContentSourceServer")
        logger.setLevel(logging.INFO)
        
        # å¦‚æœå·²ç»æœ‰å¤„ç†å™¨ï¼Œåˆ™ä¸é‡å¤æ·»åŠ 
        if logger.handlers:
            return logger
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        log_file = log_dir / "server.log"
        handler = logging.FileHandler(log_file, encoding='utf-8')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        
        logger.addHandler(handler)
        logger.addHandler(console_handler)
        
        return logger
    
    def _setup_routes(self):
        """è®¾ç½®APIè·¯ç”±"""
        
        @self.app.get("/")
        async def root(request: Request):
            """ä¸»é¡µ - è¿”å›Webç•Œé¢"""
            return self.templates.TemplateResponse("index.html", {"request": request})
        
        @self.app.get("/api")
        async def api_info():
            """APIä¿¡æ¯ - æœåŠ¡å™¨çŠ¶æ€"""
            return {
                "service": "ç½‘é¡µçˆ¬å–å’ŒAIåˆ†ææœåŠ¡",
                "status": "è¿è¡Œä¸­",
                "version": "1.0.0",
                "endpoints": {
                    "/crawl": "POST - çˆ¬å–ç½‘é¡µå¹¶è¿›è¡ŒAIåˆ†æ",
                    "/crawl_simple": "GET - ç®€å•çˆ¬å–æ¥å£ï¼ˆURLå‚æ•°ï¼‰",
                    "/health": "GET - å¥åº·æ£€æŸ¥",
                    "/providers": "GET - è·å–å¯ç”¨çš„AIæä¾›å•†",
                    "/modes": "GET - è·å–å¯ç”¨çš„AIåˆ†ææ¨¡å¼",
                    "/api/history": "GET - è·å–çˆ¬å–å†å²è®°å½•",
                    "/api/files/{task_id}": "GET - è·å–ä»»åŠ¡æ–‡ä»¶åˆ—è¡¨",
                    "/api/preview/{file_id}": "GET - é¢„è§ˆæ–‡ä»¶å†…å®¹"
                }
            }
        
        @self.app.get("/health")
        async def health_check():
            """å¥åº·æ£€æŸ¥æ¥å£"""
            return {
                "status": "healthy",
                "timestamp": datetime.now().isoformat(),
                "ai_providers": self.ai_extractor.get_available_providers(),
                "ai_modes": self.ai_extractor.get_available_modes()
            }
        
        @self.app.get("/providers")
        async def get_providers():
            """è·å–å¯ç”¨çš„AIæä¾›å•†"""
            return {
                "providers": self.ai_extractor.get_available_providers(),
                "default_provider": self.ai_extractor.config.get("llm", {}).get("default_provider", "openai")
            }
        
        @self.app.get("/modes")
        async def get_modes():
            """è·å–å¯ç”¨çš„AIåˆ†ææ¨¡å¼"""
            return {
                "modes": self.ai_extractor.get_available_modes(),
                "default_mode": self.ai_extractor.config.get("extraction", {}).get("default_mode", "structured_data")
            }
        
        @self.app.post("/crawl", response_model=CrawlResponse)
        async def crawl_and_analyze(request: CrawlRequest):
            """
            çˆ¬å–ç½‘é¡µå¹¶è¿›è¡ŒAIåˆ†æï¼ˆå®Œæ•´åŠŸèƒ½ï¼‰
            
            Args:
                request: çˆ¬å–è¯·æ±‚å‚æ•°
                
            Returns:
                çˆ¬å–å’Œåˆ†æç»“æœ
            """
            return await self._process_crawl_request(
                url=str(request.url),
                content_source=request.content_source,
                ai_modes=request.ai_modes,
                save_files=request.save_files
            )
        
        @self.app.get("/crawl_simple", response_model=CrawlResponse)
        async def crawl_simple(
            url: str = Query(..., description="è¦çˆ¬å–çš„ç½‘é¡µURL"),
            content_source: str = Query("cleaned_html", description="å†…å®¹æºç±»å‹"),
            ai_modes: str = Query("content_summary", description="AIåˆ†ææ¨¡å¼ï¼ˆé€—å·åˆ†éš”ï¼‰"),
            save_files: bool = Query(True, description="æ˜¯å¦ä¿å­˜æ–‡ä»¶")
        ) -> CrawlResponse:
            """
            ç®€å•çˆ¬å–æ¥å£ï¼ˆGETè¯·æ±‚ï¼ŒURLå‚æ•°ï¼‰
            
            Args:
                url: è¦çˆ¬å–çš„ç½‘é¡µURL
                content_source: å†…å®¹æºç±»å‹
                ai_modes: AIåˆ†ææ¨¡å¼ï¼ˆé€—å·åˆ†éš”ï¼‰
                save_files: æ˜¯å¦ä¿å­˜æ–‡ä»¶
                
            Returns:
                çˆ¬å–å’Œåˆ†æç»“æœ
            """
            # è§£æAIæ¨¡å¼
            modes_list = [mode.strip() for mode in ai_modes.split(",") if mode.strip()]
            
            return await self._process_crawl_request(
                url=url,
                content_source=content_source,
                ai_modes=modes_list,
                save_files=save_files
            )
        
        @self.app.get("/api/history")
        async def get_crawl_history(
            limit: int = Query(50, description="è¿”å›è®°å½•æ•°é‡é™åˆ¶"),
            offset: int = Query(0, description="åç§»é‡")
        ):
            """è·å–çˆ¬å–å†å²è®°å½•"""
            try:
                with get_db() as db:
                    # æŸ¥è¯¢çˆ¬å–ä»»åŠ¡è®°å½•
                    tasks = db.query(CrawlTask).order_by(CrawlTask.created_at.desc()).offset(offset).limit(limit).all()
                    
                    history = []
                    for task in tasks:
                        # è·å–ä»»åŠ¡å…³è”çš„æ–‡ä»¶æ•°é‡
                        file_count = db.query(CrawlFile).filter(CrawlFile.task_id == task.id).count()
                        
                        history.append({
                            "id": task.id,
                            "url": task.url,
                            "content_source": task.content_source,
                            "ai_modes": task.ai_modes.split(",") if task.ai_modes else [],
                            "status": task.status,
                            "created_at": task.created_at.isoformat(),
                            "file_count": file_count
                        })
                    
                    return {
                        "success": True,
                        "data": history,
                        "total": len(history)
                    }
                    
            except Exception as e:
                self.logger.error(f"è·å–å†å²è®°å½•å¤±è´¥: {str(e)}")
                raise HTTPException(status_code=500, detail=f"è·å–å†å²è®°å½•å¤±è´¥: {str(e)}")
        
        @self.app.get("/api/files/{task_id}")
        async def get_task_files(task_id: int):
            """è·å–æŒ‡å®šä»»åŠ¡çš„æ–‡ä»¶åˆ—è¡¨"""
            try:
                with get_db() as db:
                    # æŸ¥è¯¢ä»»åŠ¡ä¿¡æ¯
                    task = db.query(CrawlTask).filter(CrawlTask.id == task_id).first()
                    if not task:
                        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
                    
                    # æŸ¥è¯¢ä»»åŠ¡å…³è”çš„æ–‡ä»¶
                    files = db.query(CrawlFile).filter(CrawlFile.task_id == task_id).all()
                    
                    file_list = []
                    for file in files:
                        file_list.append({
                            "id": file.id,
                            "filename": file.filename,
                            "file_type": file.file_type,
                            "file_size": file.file_size,
                            "minio_bucket": file.minio_bucket,
                            "minio_path": file.minio_path,
                            "created_at": file.created_at.isoformat()
                        })
                    
                    return {
                        "success": True,
                        "task": {
                            "id": task.id,
                            "url": task.url,
                            "content_source": task.content_source,
                            "created_at": task.created_at.isoformat()
                        },
                        "files": file_list
                    }
                    
            except HTTPException:
                raise
            except Exception as e:
                self.logger.error(f"è·å–ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {str(e)}")
                raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        @self.app.get("/api/preview/{file_id}")
        async def preview_file(file_id: int):
            """é¢„è§ˆæ–‡ä»¶å†…å®¹"""
            try:
                with get_db() as db:
                    # æŸ¥è¯¢æ–‡ä»¶ä¿¡æ¯
                    file = db.query(CrawlFile).filter(CrawlFile.id == file_id).first()
                    if not file:
                        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
                    
                    # ä»MinIOè·å–æ–‡ä»¶å†…å®¹
                    try:
                        file_content = minio_manager.get_file_content(file.minio_bucket, file.minio_path)
                        
                        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†å†…å®¹
                        if file.file_type in ['markdown', 'json', 'txt']:
                            # æ–‡æœ¬æ–‡ä»¶ç›´æ¥è¿”å›å†…å®¹
                            content = file_content.decode('utf-8')
                        else:
                            # å…¶ä»–æ–‡ä»¶ç±»å‹è¿”å›base64ç¼–ç 
                            import base64
                            content = base64.b64encode(file_content).decode('utf-8')
                        
                        return {
                            "success": True,
                            "file": {
                                "id": file.id,
                                "filename": file.filename,
                                "file_type": file.file_type,
                                "file_size": file.file_size,
                                "content": content,
                                "is_text": file.file_type in ['markdown', 'json', 'txt']
                            }
                        }
                        
                    except Exception as e:
                        self.logger.error(f"ä»MinIOè·å–æ–‡ä»¶å†…å®¹å¤±è´¥: {str(e)}")
                        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶å†…å®¹å¤±è´¥: {str(e)}")
                    
            except HTTPException:
                raise
            except Exception as e:
                self.logger.error(f"é¢„è§ˆæ–‡ä»¶å¤±è´¥: {str(e)}")
                raise HTTPException(status_code=500, detail=f"é¢„è§ˆæ–‡ä»¶å¤±è´¥: {str(e)}")
    
    async def _process_crawl_request(
        self,
        url: str,
        content_source: str = "cleaned_html",
        ai_modes: list[str] = None,
        save_files: bool = True
    ) -> CrawlResponse:
        """
        å¤„ç†çˆ¬å–è¯·æ±‚çš„æ ¸å¿ƒé€»è¾‘
        
        Args:
            url: è¦çˆ¬å–çš„ç½‘é¡µURL
            content_source: å†…å®¹æºç±»å‹
            ai_modes: AIåˆ†ææ¨¡å¼åˆ—è¡¨
            save_files: æ˜¯å¦ä¿å­˜æ–‡ä»¶
            
        Returns:
            çˆ¬å–å’Œåˆ†æç»“æœ
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹å¤„ç†çˆ¬å–è¯·æ±‚ - URL: {url}")
            
            # éªŒè¯URLæ ¼å¼
            parsed_url = urlparse(url)
            if not parsed_url.scheme or not parsed_url.netloc:
                raise HTTPException(status_code=400, detail="æ— æ•ˆçš„URLæ ¼å¼")
            
            # è®¾ç½®é»˜è®¤AIæ¨¡å¼
            if ai_modes is None:
                ai_modes = ["structured_data", "content_summary"]
            
            # éªŒè¯AIæ¨¡å¼
            available_modes = self.ai_extractor.get_available_modes()
            invalid_modes = [mode for mode in ai_modes if mode not in available_modes]
            if invalid_modes:
                raise HTTPException(
                    status_code=400, 
                    detail=f"æ— æ•ˆçš„AIåˆ†ææ¨¡å¼: {invalid_modes}ï¼Œå¯ç”¨æ¨¡å¼: {available_modes}"
                )
            
            # åˆ›å»ºMarkdownç”Ÿæˆå™¨
            markdown_generator = DefaultMarkdownGenerator(content_source=content_source)
            config = CrawlerRunConfig(markdown_generator=markdown_generator)
            
            # æ‰§è¡Œç½‘é¡µçˆ¬å–
            self.logger.info(f"ğŸ“„ å¼€å§‹çˆ¬å–ç½‘é¡µ - å†…å®¹æº: {content_source}")
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url=url, config=config)
                
                if not result.success:
                    raise HTTPException(status_code=500, detail=f"ç½‘é¡µçˆ¬å–å¤±è´¥: {result.error_message}")
                
                markdown_content = result.markdown
                self.logger.info(f"âœ… ç½‘é¡µçˆ¬å–æˆåŠŸ - å†…å®¹é•¿åº¦: {len(markdown_content)}")
            
            # æ‰§è¡ŒAIåˆ†æ
            ai_results = {}
            if ai_modes:
                self.logger.info(f"ğŸ¤– å¼€å§‹AIåˆ†æ - æ¨¡å¼: {ai_modes}")
                for mode in ai_modes:
                    try:
                        ai_result = await self.ai_extractor.extract(markdown_content, mode=mode)
                        ai_results[mode] = ai_result
                        self.logger.info(f"âœ… AIåˆ†æå®Œæˆ - æ¨¡å¼: {mode}")
                    except Exception as e:
                        self.logger.error(f"âŒ AIåˆ†æå¤±è´¥ - æ¨¡å¼: {mode}, é”™è¯¯: {str(e)}")
                        ai_results[mode] = {
                            "success": False,
                            "error": str(e),
                            "mode": mode
                        }
            
            # ä¿å­˜æ–‡ä»¶åˆ°MinIOå’Œæ•°æ®åº“
            storage_info = {}
            if save_files:
                storage_info = await self._save_results(
                    url=url,
                    timestamp=timestamp,
                    markdown_content=markdown_content,
                    ai_results=ai_results,
                    content_source=content_source
                )
            
            self.logger.info(f"ğŸ‰ è¯·æ±‚å¤„ç†å®Œæˆ - URL: {url}")
            
            return CrawlResponse(
                success=True,
                url=url,
                timestamp=timestamp,
                markdown_content=markdown_content,
                ai_results=ai_results,
                storage_info=storage_info
            )
            
        except HTTPException:
            raise
        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†è¯·æ±‚å¤±è´¥ - URL: {url}, é”™è¯¯: {str(e)}")
            return CrawlResponse(
                success=False,
                url=url,
                timestamp=timestamp,
                error=str(e)
            )
    
    async def _save_results(
        self,
        url: str,
        timestamp: str,
        markdown_content: str,
        ai_results: Dict[str, Any],
        content_source: str
    ) -> Dict[str, Any]:
        """
        ä¿å­˜çˆ¬å–å’Œåˆ†æç»“æœåˆ°MinIOå’Œæ•°æ®åº“
        
        Args:
            url: åŸå§‹URL
            timestamp: æ—¶é—´æˆ³
            markdown_content: Markdownå†…å®¹
            ai_results: AIåˆ†æç»“æœ
            content_source: å†…å®¹æºç±»å‹
            
        Returns:
            åŒ…å«æ–‡ä»¶ä¿¡æ¯å’Œæ•°æ®åº“è®°å½•çš„å­—å…¸
        """
        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜æ–‡ä»¶
            temp_dir = Path("temp") / f"server_results_{timestamp}"
            temp_dir.mkdir(parents=True, exist_ok=True)
            
            saved_files = []
            minio_urls = []
            
            # 1. ä¿å­˜Markdownæ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            markdown_filename = f"markdown_{content_source}_{timestamp}.md"
            markdown_file = temp_dir / markdown_filename
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(f"# ç½‘é¡µçˆ¬å–ç»“æœ\n\n")
                f.write(f"**URL:** {url}\n\n")
                f.write(f"**çˆ¬å–æ—¶é—´:** {timestamp}\n\n")
                f.write(f"**å†…å®¹æº:** {content_source}\n\n")
                f.write("---\n\n")
                f.write(markdown_content)
            
            # ä¸Šä¼ Markdownæ–‡ä»¶åˆ°MinIO
            markdown_minio_url = minio_manager.upload_file(
                str(markdown_file), 
                "crawl4ai-markdown", 
                markdown_filename
            )
            if markdown_minio_url:
                minio_urls.append({
                    "type": "markdown",
                    "filename": markdown_filename,
                    "url": markdown_minio_url,
                    "size": os.path.getsize(markdown_file)
                })
                self.logger.info(f"ğŸ’¾ å·²ä¸Šä¼ Markdownæ–‡ä»¶åˆ°MinIO: {markdown_minio_url}")
            
            # 2. ä¿å­˜AIåˆ†æç»“æœ
            for mode, result in ai_results.items():
                if result.get("success"):
                    ai_filename = f"ai_{mode}_{timestamp}.md"
                    ai_file = temp_dir / ai_filename
                    with open(ai_file, 'w', encoding='utf-8') as f:
                        f.write(f"# AIåˆ†æç»“æœ - {mode}\n\n")
                        f.write(f"**URL:** {url}\n\n")
                        f.write(f"**åˆ†ææ—¶é—´:** {timestamp}\n\n")
                        f.write(f"**åˆ†ææ¨¡å¼:** {mode}\n\n")
                        f.write(f"**AIæä¾›å•†:** {result.get('provider', 'unknown')}\n\n")
                        f.write("---\n\n")
                        f.write(result.get("content", ""))
                    
                    # ä¸Šä¼ AIåˆ†ææ–‡ä»¶åˆ°MinIO
                    ai_minio_url = minio_manager.upload_file(
                        str(ai_file), 
                        "crawl4ai-ai-results", 
                        ai_filename
                    )
                    if ai_minio_url:
                        minio_urls.append({
                            "type": f"ai_{mode}",
                            "filename": ai_filename,
                            "url": ai_minio_url,
                            "size": os.path.getsize(ai_file)
                        })
                        self.logger.info(f"ğŸ’¾ å·²ä¸Šä¼ AIåˆ†ææ–‡ä»¶åˆ°MinIO: {ai_minio_url}")
            
            # 3. ä¿å­˜å®Œæ•´ç»“æœJSON
            json_filename = f"complete_results_{timestamp}.json"
            json_file = temp_dir / json_filename
            complete_results = {
                "url": url,
                "timestamp": timestamp,
                "content_source": content_source,
                "markdown_content": markdown_content,
                "ai_results": ai_results,
                "minio_files": minio_urls
            }
            
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(complete_results, f, ensure_ascii=False, indent=2)
            
            # ä¸Šä¼ JSONæ–‡ä»¶åˆ°MinIO
            json_minio_url = minio_manager.upload_file(
                str(json_file), 
                "crawl4ai-json", 
                json_filename
            )
            if json_minio_url:
                minio_urls.append({
                    "type": "json",
                    "filename": json_filename,
                    "url": json_minio_url,
                    "size": os.path.getsize(json_file)
                })
                self.logger.info(f"ğŸ’¾ å·²ä¸Šä¼ å®Œæ•´ç»“æœJSONåˆ°MinIO: {json_minio_url}")
            
            # 4. ä¿å­˜è®°å½•åˆ°æ•°æ®åº“
            db_record_id = await self._save_to_database(url, timestamp, content_source, minio_urls)
            
            # 5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)
            
            return {
                "database_id": db_record_id,
                "minio_files": minio_urls,
                "total_files": len(minio_urls)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
            return {"error": str(e)}
    
    async def _save_to_database(self, url: str, timestamp: str, content_source: str, minio_files: list) -> Optional[int]:
        """ä¿å­˜çˆ¬å–ä»»åŠ¡å’Œæ–‡ä»¶è®°å½•åˆ°æ•°æ®åº“"""
        try:
            db = next(get_db())
            
            # åˆ›å»ºçˆ¬å–ä»»åŠ¡è®°å½•
            crawl_task = CrawlTask(
                url=url,
                content_source=content_source,
                status="completed",
                created_at=datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            )
            db.add(crawl_task)
            db.commit()
            db.refresh(crawl_task)
            
            # åˆ›å»ºæ–‡ä»¶è®°å½•
            for file_info in minio_files:
                crawl_file = CrawlFile(
                    task_id=crawl_task.id,
                    filename=file_info["filename"],
                    file_type=file_info["type"],
                    file_size=file_info["size"],
                    minio_url=file_info["url"],
                    file_metadata={"content_source": content_source}
                )
                db.add(crawl_file)
            
            db.commit()
            self.logger.info(f"ğŸ’¾ å·²ä¿å­˜æ•°æ®åº“è®°å½•ï¼Œä»»åŠ¡ID: {crawl_task.id}")
            return crawl_task.id
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æ•°æ®åº“è®°å½•å¤±è´¥: {str(e)}")
            if 'db' in locals():
                db.rollback()
            return None
        finally:
            if 'db' in locals():
                db.close()
    
    def run(self):
        """å¯åŠ¨æœåŠ¡å™¨"""
        try:
            self.logger.info(f"ğŸš€ å¯åŠ¨ç½‘é¡µçˆ¬å–å’ŒAIåˆ†ææœåŠ¡å™¨")
            self.logger.info(f"ğŸ“¡ æœåŠ¡å™¨åœ°å€: http://{self.host}:{self.port}")
            self.logger.info(f"ğŸ“š APIæ–‡æ¡£: http://{self.host}:{self.port}/docs")
            self.logger.info(f"ğŸ”§ å¯ç”¨AIæä¾›å•†: {self.ai_extractor.get_available_providers()}")
            self.logger.info(f"ğŸ¯ å¯ç”¨AIæ¨¡å¼: {self.ai_extractor.get_available_modes()}")
            
            # æ·»åŠ å¯åŠ¨å‰çš„æ—¥å¿—
            self.logger.info("ğŸ”„ æ­£åœ¨å¯åŠ¨UvicornæœåŠ¡å™¨...")
            
            uvicorn.run(
                self.app,
                host=self.host,
                port=self.port,
                log_level="info"
            )
        except Exception as e:
            self.logger.error(f"âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {str(e)}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    # ä»ç»Ÿä¸€é…ç½®è·å–é»˜è®¤å€¼
    from config.config_loader import config_loader
    
    try:
        # åŠ è½½é…ç½®
        config_loader.load_config()
        server_config = config_loader.get_server_config()
        default_host = server_config.get('host', '0.0.0.0')
        default_port = server_config.get('port', 8080)
    except Exception as e:
        print(f"âš ï¸ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
        default_host = '0.0.0.0'
        default_port = 8080
    
    parser = argparse.ArgumentParser(description="ç½‘é¡µçˆ¬å–å’ŒAIåˆ†æHTTPæœåŠ¡å™¨")
    parser.add_argument("--host", default=default_host, help="æœåŠ¡å™¨ä¸»æœºåœ°å€")
    parser.add_argument("--port", type=int, default=default_port, help="æœåŠ¡å™¨ç«¯å£")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¹¶å¯åŠ¨æœåŠ¡å™¨
    server = ContentSourceServer(host=args.host, port=args.port)
    server.run()


if __name__ == "__main__":
    main()